{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-07-30T16:17:30.273208Z","iopub.status.busy":"2023-07-30T16:17:30.272233Z","iopub.status.idle":"2023-07-30T16:17:40.387310Z","shell.execute_reply":"2023-07-30T16:17:40.385628Z","shell.execute_reply.started":"2023-07-30T16:17:30.273166Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/andriievskyi/miniforge3/envs/pytorchenv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import transformers\n","import torch\n","import torch.nn as nn\n","import torch.optim as opt\n","from torch.nn import functional as F\n","from datasets import load_dataset\n","from transformers import BertTokenizer\n","from torch.utils.data import Dataset, DataLoader\n","\n","from functools import partial"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["DEVICE=\"cpu\""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["\n","\n","# Define the text dataset\n","text_dataset_en = [\n","    \"The quick brown fox jumps over the lazy dog and runs through the forest.\",\n","    \"She sells sea shells by the seashore and enjoys watching the waves crash.\",\n","    \"He plays the guitar and sings beautiful songs under the shining moon.\",\n","    \"They build sandcastles on the beach and fly colorful kites in the sky.\",\n","    \"We explore ancient ruins and discover hidden treasures in mysterious caves.\",\n","    \"The sun sets behind the mountains, painting the sky with hues of pink and orange.\",\n","    \"A gentle breeze rustles the leaves on the trees as birds sing their sweet melodies.\",\n","    \"Children laugh and play in the park, enjoying the warmth of a sunny day.\",\n","    \"In the distance, a train whistle blows, signaling the arrival of a new adventure.\",\n","    \"As night falls, stars twinkle in the dark sky, guiding travelers on their journey.\",\n","]\n","\n","text_dataset_ua = [\n","    \"Швидкий коричневий лис стрибає через лінивого собаку та біжить через ліс.\",\n","    \"Вона продає морські ракушки біля морського узбережжя та насолоджується спостереженням за хвилями.\",\n","    \"Він грає на гітарі та співає прекрасні пісні під сяючою місяцем.\",\n","    \"Вони будують піщані замки на пляжі та літають різнокольоровими паперовими зміями на небі.\",\n","    \"Ми досліджуємо стародавні руїни та відкриваємо приховані скарби в таємничих печерах.\",\n","    \"Сонце заходить за гори, розфарбовуючи небо рожевим та помаранчевим.\",\n","    \"Легкий бриз шелестить листям на деревах, поки птахи співають свої чарівні мелодії.\",\n","    \"Діти сміються та граються в парку, насолоджуючись теплотою сонячного дня.\",\n","    \"Удачний випадок, здалеку чути свисток потяга, сигналізуючий про прибуття нової пригоди.\",\n","    \"По вечорах зірки мерехтять на темному небі, направляючи подорожуючих на їхній шлях.\",\n","]"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Initialize the english tokenizer\n","tokenizer_en = BertTokenizer.from_pretrained(\"bert-base-cased\")\n","\n","# Initialize the ukrainian tokenizer\n","tokenizer_ua = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n","\n","def tokenize_data(en_data, ua_data, tokenizer_en=None, tokenizer_ua=None, max_length=30):\n","    if not tokenizer_en and not tokenizer_ua:\n","        # Initialize the english tokenizer\n","        tokenizer_en = BertTokenizer.from_pretrained(\"bert-base-cased\")\n","\n","        # Initialize the ukrainian tokenizer\n","        tokenizer_ua = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n","\n","    # Tokenize the data\n","    en_tokenized_data  = [tokenizer_en(text, padding=\"max_length\", max_length=max_length, truncation=True, return_tensors=\"pt\") for text in en_data]\n","    ua_tokenized_data = [tokenizer_ua(text, padding=\"max_length\", max_length=max_length, truncation=True, return_tensors=\"pt\") for text in ua_data]\n","\n","    return en_tokenized_data, ua_tokenized_data"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[  101,  1109,  3613,  3058, 17594, 15457,  1166,  1103, 16688,  3676,\n","          1105,  2326,  1194,  1103,  3304,   119,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["en_tokenized_data, ua_tokenized_data = tokenize_data(text_dataset_en, text_dataset_ua, tokenizer_en, tokenizer_ua)\n","en_tokenized_data[0][\"input_ids\"][:10]"]},{"cell_type":"markdown","metadata":{},"source":["## Creating a custom dataset"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["class TranslationDataset(Dataset):\n","    def __init__(self, src_data, trg_data, src_tokenizer, trg_tokenizer, max_length=30):\n","        super().__init__()\n","        # Tokenize th data\n","        self.src_tokenized_data, self.trg_tokenized_data = tokenize_data(src_data, trg_data, src_tokenizer, trg_tokenizer, max_length)\n","    \n","    def __len__(self):\n","        return len(self.src_tokenized_data)\n","    \n","    def __getitem__(self, idx):\n","        src_sample = {\n","            \"input_ids\": self.src_tokenized_data[idx][\"input_ids\"],\n","            \"attention_mask\": self.src_tokenized_data[idx][\"attention_mask\"]\n","        }\n","        trg_sample = {\n","            \"input_ids\": self.trg_tokenized_data[idx][\"input_ids\"],\n","            \"attention_mask\": self.trg_tokenized_data[idx][\"attention_mask\"]\n","        }\n","\n","        return src_sample, trg_sample"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["translation_dataset = TranslationDataset(text_dataset_en, text_dataset_ua, tokenizer_en, tokenizer_ua)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Create the data loader\n","batch_size = 32\n","data_loader = DataLoader(translation_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":277,"metadata":{},"outputs":[],"source":["class Head(nn.Module):\n","    def __init__(self, n_embed, head_size, block_size, masked=False):\n","        super().__init__()\n","        self.query = nn.Linear(n_embed, head_size, bias=False)\n","        self.key = nn.Linear(n_embed, head_size, bias=False)\n","        self.value = nn.Linear(n_embed, head_size, bias=False)\n","        self.masked = masked\n","        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n","        \n","    def forward(self, q, k, v):\n","        B, T, C = q.shape\n","        q = self.query(q)\n","        k = self.key(k)\n","        v = self.value(v)\n","                            \n","        # Compute scaled dot-product attention\n","        dot_product = torch.matmul(q, k.transpose(-2, -1))\n","        scaled_dot_product = dot_product * C ** -0.5 # divide by the square root of d_k\n","    \n","        # Apply masking if it is a decoder\n","        w = (scaled_dot_product.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) \n","            if self.masked else scaled_dot_product)\n","\n","        w = F.softmax(w, dim=1)\n","        w = torch.matmul(w, v)\n","        return w\n","    \n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, n_embed, n_heads, head_size, block_size, masked=False):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(n_embed, head_size, block_size, masked) for i in range(n_heads)])\n","        self.projection = nn.Linear(n_embed, n_embed)\n","        \n","    def forward(self, q, k, v):\n","        x = torch.cat([h(q, k, v) for h in self.heads], dim=-1)\n","        x = self.projection(x)\n","        return x\n","    \n","class FeedForward(nn.Module):\n","    def __init__(self, n_embed, dropout_p):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embed, 4 * n_embed),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embed, n_embed))\n","    \n","    def forward(self, x):\n","        return self.net(x)\n","    \n","class EncoderBlock(nn.Module):\n","    def __init__(self, n_embed, n_heads, block_size, dropout_p):\n","        super().__init__()\n","        head_size = n_embed // n_heads\n","        self.sa = MultiHeadAttention(n_embed, n_heads, head_size, block_size)\n","        self.ffwd = FeedForward(n_embed, dropout_p) \n","        self.norm1 = nn.LayerNorm(n_embed)\n","        self.norm2 = nn.LayerNorm(n_embed)\n","    \n","    def forward(self, x):\n","        # Residual connections\n","        x = self.norm1(x + self.sa(x, x, x))\n","        x = self.norm2(x + self.ffwd(x))\n","        return x\n","    \n","class Encoder(nn.Module):\n","    def __init__(self, vocab_size, n_embed, n_heads, block_size, n_layers, dropout_p):\n","        super().__init__()\n","        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n","        self.pos_embedding = nn.Embedding(block_size, n_embed)\n","        self.blocks = nn.Sequential(*[\n","            EncoderBlock(n_embed, n_heads, block_size, dropout_p) for _ in range(n_layers)\n","        ])       \n","\n","    def forward(self, x):\n","        B, T = x.shape\n","        token_embed = self.token_embedding(x)\n","        positional_embed = self.pos_embedding(torch.arange(T, device=DEVICE))\n","        embedding = token_embed + positional_embed\n","        x = self.blocks(embedding)\n","        return x\n","    \n","class DecoderBlock(nn.Module):\n","    \"\"\"Decoder Block\"\"\"\n","    def __init__(self, n_embed, n_heads, block_size, dropout_p):\n","        super().__init__()\n","        head_size = n_embed // n_heads\n","        self.sa = MultiHeadAttention(n_embed, n_heads, head_size, block_size, masked=True) # masked self-attention\n","        self.encoder_decoder_attention = MultiHeadAttention(n_embed, n_heads, head_size, block_size)\n","        self.ffwd = FeedForward(n_embed, dropout_p)\n","        self.norm1 = nn.LayerNorm(n_embed)\n","        self.norm2 = nn.LayerNorm(n_embed)\n","        self.norm3 = nn.LayerNorm(n_embed)\n","\n","    def forward(self, x, encoder_output):\n","        x = self.norm1(x + self.sa(x, x, x))\n","        x = self.norm2(x + self.encoder_decoder_attention(x, encoder_output, encoder_output))\n","        x = self.norm3(x + self.ffwd(x))\n","        return x\n","    \n","class Decoder(nn.Module):\n","    def __init__(self, vocab_size, n_embed, n_heads, block_size, n_layers, dropout_p):\n","        super().__init__()\n","        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n","        self.pos_embedding = nn.Embedding(block_size, n_embed)\n","        self.blocks = nn.ModuleList([\n","            DecoderBlock(n_embed, n_heads, block_size, dropout_p) for _ in range(n_layers)\n","        ])\n","        self.lm_head = nn.Linear(n_embed, vocab_size)\n","\n","    def forward(self, x, enc_out):\n","        B, T= x.shape\n","        token_embed = self.token_embedding(x)\n","        pos_embed = self.pos_embedding(torch.arange(T, device=DEVICE))\n","        embedding = token_embed + pos_embed\n","\n","        x = embedding\n","        for block in self.blocks:\n","            x = block(x, enc_out)\n","\n","        x = self.lm_head(x)\n","        return x\n","\n","class Transformer(nn.Module):\n","    def __init__(self, src_vocab_size, trgt_vocab_size, n_embed, n_heads, block_size, n_layers, dropout_p):\n","        super().__init__()\n","        self.encoder = Encoder(src_vocab_size, n_embed, n_heads, block_size, n_layers, dropout_p)\n","        self.decoder = Decoder(trgt_vocab_size, n_embed, n_heads, block_size, n_layers, dropout_p)\n","\n","    def forward(self, x, y):\n","        encoded = self.encoder(x)\n","        out = self.decoder(y, encoded)\n","        return out"]},{"cell_type":"code","execution_count":270,"metadata":{},"outputs":[],"source":["vocab_size_en = tokenizer_en.vocab_size\n","vocab_size_ua = tokenizer_ua.vocab_size"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":4}
